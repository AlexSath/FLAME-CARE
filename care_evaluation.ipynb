{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e475919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, logging\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tifffile as tiff\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from flame import CAREInferenceSession\n",
    "from flame.utils import get_input_and_GT_paths, _compress_dict_fields\n",
    "import flame.eval as eval\n",
    "from flame.error import FLAMEEvalError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eb6bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"20250618_224I_denoising_5to40F\"\n",
    "DATASET_DIREC = os.path.join(\"/mnt/d/data/processed\", DATASET_NAME)\n",
    "DATASET_ID = \"0x0003\"\n",
    "TEST_DIREC = os.path.join(DATASET_DIREC, \"test\")\n",
    "METRICS = [\n",
    "    \"mse\",\n",
    "    \"mae\",\n",
    "    \"ssim\"\n",
    "]\n",
    "TRACKING_URI = \"http://127.0.0.1:5050\"\n",
    "MLFLOW_RUN_ID = \"bf9a43f3ec154c9ba2deb6de2fb0db33\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ab5827",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(uri=TRACKING_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2f28ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"main\")\n",
    "logging.basicConfig(\n",
    "    filename=os.path.join(os.getcwd(), \"logs\", f\"{datetime.now().strftime('%Y%m%d-%H%M%S')}_logger.log\"),\n",
    "    encoding=\"utf-8\",\n",
    "    level=logging.DEBUG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89215a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.isdir(TEST_DIREC), f\"Could not find test set directory at path {TEST_DIREC}\"\n",
    "assert os.path.isdir(DATASET_DIREC), f\"Could not find dataset directory at path {DATASET_DIREC}\"\n",
    "for metric in METRICS:\n",
    "    try:\n",
    "        getattr(eval, metric)\n",
    "    except AttributeError as e:\n",
    "        logger.error(f\"Could not find {metric} among available evaluation metrics.\")\n",
    "        raise FLAMEEvalError(f\"Could not find {metric} among available evaluation metrics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cd75d2",
   "metadata": {},
   "source": [
    "### Getting MLFlow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4c66f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = CAREInferenceSession.from_mlflow_uri(\n",
    "    tracking_uri=TRACKING_URI,\n",
    "    run_id=MLFLOW_RUN_ID,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7f6b2e",
   "metadata": {},
   "source": [
    "### Starting Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f610859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = engine.model_config\n",
    "FRAMES_LOW = config['FLAME_Dataset']['input']['n_frames']\n",
    "FRAMES_GT = config['FLAME_Dataset']['output']['n_frames']\n",
    "low_paths, GT_paths = get_input_and_GT_paths(\n",
    "    input_direc=TEST_DIREC,\n",
    "    input_frames=FRAMES_LOW,\n",
    "    gt_frames=FRAMES_GT,\n",
    "    logger=logger\n",
    ")\n",
    "# config['FLAME_Dataset']['id'] = DATASET_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a99703",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_metrics = {x: [] for x in METRICS}\n",
    "eval_metrics = {x: [] for x in METRICS}\n",
    "\n",
    "for low_path, gt_path in tqdm(\n",
    "        iterable=zip(low_paths, GT_paths),\n",
    "        total=len(low_paths),\n",
    "        ascii=True\n",
    "    ):\n",
    "    try:\n",
    "        t1 = time()\n",
    "        low=tiff.imread(low_path).transpose(0,2,3,1).astype(np.float32)\n",
    "        gt=tiff.imread(gt_path).transpose(0,2,3,1).astype(np.float32)\n",
    "        t2 = time()\n",
    "        logger.info(f\"Loaded 2 images, taking {t2 - t1:.2f}s.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Could not load input and/or GT images from {os.path.basename(low_path)} & {os.path.basename(gt_path)}\")\n",
    "        continue\n",
    "    \n",
    "    assert low.shape == gt.shape, f\"Input and GT image shapes do not match (found {low.shape} and {gt.shape})\"\n",
    "\n",
    "    pred = engine.predict(low).astype(np.float32)\n",
    "\n",
    "    for metric in METRICS:\n",
    "        input_metrics[metric].append(getattr(eval, metric)(low[0,...], gt[0,...]))\n",
    "        eval_metrics[metric].append(getattr(eval, metric)(pred[0,...], gt[0,...]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef913c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=input_metrics)\n",
    "df[\"source\"] = [\"input vs. gt\"] * len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c90457",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame(data=eval_metrics)\n",
    "eval_df[\"source\"] = [\"pred vs. gt\"] * len(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d95caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat([df, eval_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc31261",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_id=MLFLOW_RUN_ID):\n",
    "    mlflow.log_params(_compress_dict_fields(config))\n",
    "    for metric in METRICS:\n",
    "        sns.catplot(data=all_df, x=\"source\", y=metric)\n",
    "        mlflow.log_metric(f\"ds{DATASET_ID}_test_{metric}\", np.mean(eval_df[metric]))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "care",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
