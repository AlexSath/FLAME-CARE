{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e475919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, logging\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tifffile as tiff\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from flame import CAREInferenceSession, FLAMEImage\n",
    "from flame.utils import _compress_dict_fields\n",
    "from flame.io import find_dataset_config, flame_paths_from_ids\n",
    "import flame.eval as eval\n",
    "from flame.error import FLAMEEvalError, CAREInferenceError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eb6bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAMEImage_ROOT_DIR = \"/mnt/d/data/raw\"\n",
    "DATASET_JSON_DIREC = os.path.join(os.getcwd(), \"datasets\")\n",
    "FLAMEImage_INDEX_PATH = os.path.join(DATASET_JSON_DIREC, \"raw_image_index.csv\")\n",
    "DATASET_ID = \"0x0003\"\n",
    "METRICS = [\n",
    "    \"mse\",\n",
    "    \"mae\",\n",
    "    \"ssim\"\n",
    "]\n",
    "FRAMES_LOW = 5\n",
    "FRAMES_GT = 40\n",
    "TRACKING_URI = \"http://127.0.0.1:5050\"\n",
    "MLFLOW_RUN_IDS = [\n",
    "    \"f6f35ad93a6a4c2b9a1a99ac7dea4094\",\n",
    "    \"bf9a43f3ec154c9ba2deb6de2fb0db33\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2f28ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"MAIN\")\n",
    "logging.basicConfig(\n",
    "    filename=os.path.join(os.getcwd(), \"logs\", f\"{datetime.now().strftime('%Y%m%d-%H%M%S')}_logger.log\"),\n",
    "    encoding=\"utf-8\",\n",
    "    level=logging.DEBUG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9fad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.isdir(FLAMEImage_ROOT_DIR), f\"Could not find FLAMEImage root directory at {FLAMEImage_ROOT_DIR}\"\n",
    "assert os.path.isdir(DATASET_JSON_DIREC), f\"Could not find the dataset directory at {DATASET_JSON_DIREC}\"\n",
    "assert os.path.isfile(FLAMEImage_INDEX_PATH), f\"Could not find FLAMEImage index at {FLAMEImage_INDEX_PATH}\"\n",
    "for metric in METRICS:\n",
    "    try:\n",
    "        getattr(eval, metric)\n",
    "    except AttributeError as e:\n",
    "        logger.error(f\"Could not find {metric} among available evaluation metrics.\")\n",
    "        raise FLAMEEvalError(f\"Could not find {metric} among available evaluation metrics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c0aced",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path, config = find_dataset_config(\n",
    "    input_direc=DATASET_JSON_DIREC,\n",
    "    this_id=DATASET_ID,\n",
    ")\n",
    "test_ids = config['FLAME_Dataset']['test_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63359adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = flame_paths_from_ids(\n",
    "    root_dir=FLAMEImage_ROOT_DIR,\n",
    "    index_path=FLAMEImage_INDEX_PATH,\n",
    "    id_list=test_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ba6d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Found {len(paths)} FLAME Images from {DATASET_ID} test set in {FLAMEImage_ROOT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daa4710",
   "metadata": {},
   "source": [
    "### Loading FLAMEImages into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e4bff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "flame_image_objects = []\n",
    "logger.info(f\"Loading FLAMEImages into memory...\")\n",
    "for p in tqdm(paths, total=len(paths), ascii=True):\n",
    "    im = FLAMEImage(\n",
    "        impath=p,\n",
    "        jsonext='tileData.txt'\n",
    "    )\n",
    "    flame_image_objects.append(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cd75d2",
   "metadata": {},
   "source": [
    "### Getting MLFlow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ab5827",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(uri=TRACKING_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4c66f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_engines = []\n",
    "for rdx, RUN_ID in enumerate(MLFLOW_RUN_IDS):\n",
    "    logger.info(f\"Evaluating run {rdx+1} / {len(MLFLOW_RUN_IDS)}...\")\n",
    "    print(f\"Evaluating run {rdx+1} / {len(MLFLOW_RUN_IDS)}...\")\n",
    "\n",
    "    try:\n",
    "        engine = CAREInferenceSession.from_mlflow_uri(\n",
    "            tracking_uri=TRACKING_URI,\n",
    "            run_id=RUN_ID,\n",
    "        )\n",
    "        inference_engines.append(engine)\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Could not initialize CAREInferenceSession from MLFlow run id {RUN_ID}.\\n{e.__class__.__name__}: {e}\")\n",
    "        raise CAREInferenceError(f\"Could not initialize CAREInferenceSession from MLFlow run id {RUN_ID}.\\n{e.__class__.__name__}: {e}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7f6b2e",
   "metadata": {},
   "source": [
    "### Inference and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a99703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Input frames should probably be dynamically sourced from the engine's config (as the engine was trained with a certain number of frames)\n",
    "\n",
    "df_dict = {\n",
    "    \"image\": [],\n",
    "    \"metric\": [],\n",
    "    \"value\": [],\n",
    "    \"run-name\": []\n",
    "}\n",
    "\n",
    "for flame_im in tqdm(\n",
    "        iterable=flame_image_objects,\n",
    "        total=len(flame_image_objects),\n",
    "        ascii=True\n",
    "    ):\n",
    "\n",
    "    flame_im.openImage()\n",
    "        \n",
    "    for engine in inference_engines:\n",
    "        try:\n",
    "            this_pred = engine.predict_FLAME(\n",
    "                image=flame_im,\n",
    "                input_frames=5\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Could not infer on {flame_im} with Inference Session {engine.mlflow_run_name if engine.from_mlflow else hex(id(engine))}\")\n",
    "            raise CAREInferenceError(f\"Could not infer on {flame_im} with Inference Session {engine.mlflow_run_name if engine.from_mlflow else hex(id(engine))}\")\n",
    "        \n",
    "        this_GT = flame_im.get_frames((0, FRAMES_GT)).astype(this_pred.dtype)\n",
    "\n",
    "        for metric in METRICS:\n",
    "            df_dict[\"image\"].append(flame_im.impath)\n",
    "            df_dict[\"metric\"].append(metric)\n",
    "            if metric == \"ssim\":\n",
    "                channel_index = flame_im.axes_shape.index(\"C\")\n",
    "                value = getattr(eval, metric)(this_pred[0,...], this_GT[0,...], channel_axis=channel_index-1)\n",
    "            else:\n",
    "                value = getattr(eval, metric)(this_pred[0,...], this_GT[0,...])\n",
    "            df_dict[\"value\"].append(value)\n",
    "            df_dict[\"run-name\"].append(engine.mlflow_run_name if engine.from_mlflow else hex(id(engine)))\n",
    "\n",
    "    flame_im.closeImage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef913c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=df_dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f57648",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in METRICS:\n",
    "    if metric == \"ssim\": continue\n",
    "    values = df.loc[df[\"metric\"] == metric, \"value\"].to_list()\n",
    "    df.loc[df[\"metric\"] == metric, \"value\"] = (values - np.min(values)) / (np.max(values) - np.min(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d37cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37445596",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "axes = sns.boxplot(data=df, x=\"metric\", y=\"value\", hue=\"run-name\")\n",
    "plt.legend(bbox_to_anchor=(1.01, 1))\n",
    "plt.ylabel(\"Relative Score (0-1 norm)\")\n",
    "plt.xlabel(\"Metric\")\n",
    "plt.title(f\"Prediction vs. Ground Truth Performance Comparison in\\nTest Set from Dataset id{DATASET_ID} ({len(flame_image_objects)} images)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "care",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
