{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e475919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, logging\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tifffile as tiff\n",
    "\n",
    "from flame import CAREInferenceSession\n",
    "from flame.utils import get_input_and_GT_paths\n",
    "import flame.eval as eval\n",
    "from flame.error import FLAMEEvalError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1eb6bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"20250618_224I_denoising_5to40F\"\n",
    "DATASET_DIREC = os.path.join(\"/mnt/d/data/processed\", DATASET_NAME)\n",
    "TEST_DIREC = os.path.join(DATASET_DIREC, \"test\")\n",
    "METRICS = [\n",
    "    \"mse\",\n",
    "    \"mae\",\n",
    "    \"ssim\"\n",
    "]\n",
    "TRACKING_URI = \"http://127.0.0.1:5050\"\n",
    "MLFLOW_RUN_ID = \"bf9a43f3ec154c9ba2deb6de2fb0db33\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50ab5827",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(uri=TRACKING_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd2f28ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"main\")\n",
    "logging.basicConfig(\n",
    "    filename=os.path.join(os.getcwd(), \"logs\", f\"{datetime.now().strftime('%Y%m%d-%H%M%S')}_logger.log\"),\n",
    "    encoding=\"utf-8\",\n",
    "    level=logging.DEBUG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89215a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.isdir(TEST_DIREC), f\"Could not find test set directory at path {TEST_DIREC}\"\n",
    "for metric in METRICS:\n",
    "    try:\n",
    "        getattr(eval, metric)\n",
    "    except AttributeError as e:\n",
    "        logger.error(f\"Could not find {metric} among available evaluation metrics.\")\n",
    "        raise FLAMEEvalError(f\"Could not find {metric} among available evaluation metrics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cd75d2",
   "metadata": {},
   "source": [
    "### Getting MLFlow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b4c66f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a78518abf3d945d4b46ee76a4ed17d3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "118e9511db4a47b9b3ec11e481f53e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** EP Error ***************\n",
      "EP Error /onnxruntime_src/onnxruntime/python/onnxruntime_pybind_state.cc:505 void onnxruntime::python::RegisterTensorRTPluginsAsCustomOps(PySessionOptions&, const onnxruntime::ProviderOptions&) Please install TensorRT libraries as mentioned in the GPU requirements page, make sure they're in the PATH or LD_LIBRARY_PATH, and that your GPU is supported.\n",
      " when using ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Falling back to ['CPUExecutionProvider'] and retrying.\n",
      "****************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;31m2025-06-20 17:35:32.792834889 [E:onnxruntime:Default, provider_bridge_ort.cc:2167 TryGetProviderInfo_TensorRT] /onnxruntime_src/onnxruntime/core/session/provider_bridge_ort.cc:1778 onnxruntime::Provider& onnxruntime::ProviderLibrary::Get() [ONNXRuntimeError] : 1 : FAIL : Failed to load library libonnxruntime_providers_tensorrt.so with error: libnvinfer.so.10: cannot open shared object file: No such file or directory\n",
      "\u001b[m\n"
     ]
    }
   ],
   "source": [
    "engine = CAREInferenceSession.from_mlflow_uri(\n",
    "    tracking_uri=TRACKING_URI,\n",
    "    run_id=MLFLOW_RUN_ID,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7f6b2e",
   "metadata": {},
   "source": [
    "### Starting Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f610859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = engine.model_config\n",
    "FRAMES_LOW = config['FLAME_Dataset']['input']['n_frames']\n",
    "FRAMES_GT = config['FLAME_Dataset']['output']['n_frames']\n",
    "low_paths, GT_paths = get_input_and_GT_paths(\n",
    "    input_direc=TEST_DIREC,\n",
    "    input_frames=FRAMES_LOW,\n",
    "    gt_frames=FRAMES_GT,\n",
    "    logger=logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94a99703",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|##########| 22/22 [00:33<00:00,  1.50s/it]\n"
     ]
    }
   ],
   "source": [
    "for low_path, gt_path in tqdm(\n",
    "        iterable=zip(low_paths, GT_paths),\n",
    "        total=len(low_paths),\n",
    "        ascii=True\n",
    "    ):\n",
    "    try:\n",
    "        t1 = time()\n",
    "        low=tiff.imread(low_path).transpose(0,2,3,1).astype(np.float32)\n",
    "        gt=tiff.imread(gt_path).transpose(0,2,3,1).astype(np.float32)\n",
    "        t2 = time()\n",
    "        logger.info(f\"Loaded 2 images, taking {t2 - t1:.2f}s.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Could not load input and/or GT images from {os.path.basename(low_path)} & {os.path.basename(gt_path)}\")\n",
    "        continue\n",
    "    \n",
    "    assert low.shape == gt.shape, f\"Input and GT image shapes do not match (found {low.shape} and {gt.shape})\"\n",
    "\n",
    "    pred = engine.predict(low)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef913c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "care",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
