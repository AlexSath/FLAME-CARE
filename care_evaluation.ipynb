{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e475919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, logging\n",
    "from datetime import datetime\n",
    "\n",
    "import mlflow\n",
    "import mlflow.artifacts as artifacts\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tifffile as tiff\n",
    "\n",
    "from flame import CAREInferenceSession\n",
    "from flame.utils import get_input_and_GT_paths\n",
    "import flame.eval as eval\n",
    "from flame.error import FLAMEEvalError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1eb6bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"20250618_224I_denoising_5to40F\"\n",
    "DATASET_DIREC = os.path.join(\"/mnt/d/data/processed\", DATASET_NAME)\n",
    "TEST_DIREC = os.path.join(DATASET_DIREC, \"test\")\n",
    "METRICS = [\n",
    "    \"mse\",\n",
    "    \"mae\",\n",
    "    \"ssim\"\n",
    "]\n",
    "TRACKING_URI = \"http://127.0.0.1:5050\"\n",
    "MLFLOW_RUN_ID = \"bf9a43f3ec154c9ba2deb6de2fb0db33\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ab5827",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(uri=TRACKING_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd2f28ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"main\")\n",
    "logging.basicConfig(\n",
    "    filename=os.path.join(os.getcwd(), \"logs\", f\"{datetime.now().strftime('%Y%m%d-%H%M%S')}_logger.log\"),\n",
    "    encoding=\"utf-8\",\n",
    "    level=logging.DEBUG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89215a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.isdir(TEST_DIREC), f\"Could not find test set directory at path {TEST_DIREC}\"\n",
    "for metric in METRICS:\n",
    "    try:\n",
    "        getattr(eval, metric)\n",
    "    except AttributeError as e:\n",
    "        logger.error(f\"Could not find {metric} among available evaluation metrics.\")\n",
    "        raise FLAMEEvalError(f\"Could not find {metric} among available evaluation metrics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cd75d2",
   "metadata": {},
   "source": [
    "### Getting MLFlow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b4c66f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b492247822a94a4ea552e1c549e72b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a0b37781b44222b8bd25d661c906d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "engine = CAREInferenceSession.from_mlflow_uri(\n",
    "    tracking_uri=TRACKING_URI,\n",
    "    run_id=MLFLOW_RUN_ID,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7f6b2e",
   "metadata": {},
   "source": [
    "### Starting Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f610859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAMES_LOW = config['FLAME_Dataset']['input']['n_frames']\n",
    "FRAMES_GT = config['FLAME_Dataset']['output']['n_frames']\n",
    "low_paths, GT_paths = get_input_and_GT_paths(\n",
    "    input_direc=TEST_DIREC,\n",
    "    input_frames=FRAMES_LOW,\n",
    "    gt_frames=FRAMES_GT,\n",
    "    logger=logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a99703",
   "metadata": {},
   "outputs": [],
   "source": [
    "for low_path, gt_path in tqdm(\n",
    "        iterable=zip(low_paths, GT_paths),\n",
    "        total=len(low_paths),\n",
    "        ascii=True\n",
    "    ):\n",
    "    try:\n",
    "        low=tiff.imread(low_path).transpose(0,2,3,1).astype(np.float32)\n",
    "        gt=tiff.imread(gt_path).transpose(0,2,3,1).astype(np.float32)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Could not load input and/or GT images from {os.path.basename(low_path)} & {os.path.basename(gt_path)}\")\n",
    "        continue\n",
    "    \n",
    "    assert low.shape == gt.shape, f\"Input and GT image shapes do not match (found {low.shape} and {gt.shape})\"\n",
    "\n",
    "    pred = engine.predict(low)\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1a8f54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "care",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
