{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a99584ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 14:45:38.168751: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-17 14:45:38.176112: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750196738.184479 1618023 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750196738.186903 1618023 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750196738.193509 1618023 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750196738.193519 1618023 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750196738.193521 1618023 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750196738.193521 1618023 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-17 14:45:38.196049: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/balulab/miniconda3/envs/care/lib/python3.12/site-packages/mlflow/pyfunc/utils/data_validation.py:155: FutureWarning: Model's `predict` method contains invalid parameters: {'kwargs', 'args'}. Only the following parameter names are allowed: context, model_input, and params. Note that invalid parameters will no longer be permitted in future versions.\n",
      "  param_names = _check_func_signature(func, \"predict\")\n",
      "/home/balulab/miniconda3/envs/care/lib/python3.12/site-packages/mlflow/pyfunc/utils/data_validation.py:186: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
      "  color_warning(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from csbdeep.models import Config, CARE\n",
    "from matplotlib import pyplot as plt\n",
    "import tifffile as tiff\n",
    "\n",
    "from flame.engine import CAREInferenceSession\n",
    "from flame import FLAMEImage\n",
    "from flame.error import FLAMEImageError\n",
    "from flame.utils import min_max_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6e18bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE_DATA_DIR = \"/mnt/d/data/raw/0013_250514_HS6307_CAREtest_NA\"\n",
    "INFERENCE_DATA_DIR = \"/mnt/d/data/raw/S268_250409_CAREtraining_PL\"\n",
    "# INFERENCE_OUTPUT = \"/mnt/d/data/output/0013_250514_HS6307_CAREtest_NA\"\n",
    "INFERENCE_OUTPUT = \"/mnt/d/data/output/S268_250409_CAREtraining_PL\"\n",
    "DATASET_DIRECTORY = \"/mnt/d/code/Balu_CARE/datasets\"\n",
    "DATASET_NAME = \"20250513_40I_denoising_7to40F\"\n",
    "DATASET_JSON = os.path.join(DATASET_DIRECTORY, f\"{DATASET_NAME}.json\")\n",
    "MODEL_DIRECTORY = \"/mnt/d/models/CARE/test_model\"\n",
    "MODEL_NAME = os.path.basename(MODEL_DIRECTORY)\n",
    "INFERENCE_OUTPUT_DIRECTORY = os.path.join(INFERENCE_OUTPUT, MODEL_NAME)\n",
    "\n",
    "ONNX_PATH = os.path.join(MODEL_DIRECTORY, f\"{MODEL_NAME}.onnx\")\n",
    "JSON_PATH = os.path.join(MODEL_DIRECTORY, f\"model_config.json\")\n",
    "\n",
    "for f in [DATASET_JSON, ONNX_PATH, JSON_PATH]:\n",
    "    assert os.path.isfile(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3c47659",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(INFERENCE_OUTPUT_DIRECTORY, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa7fad71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'bidirectionalCorrection' could not be loaded from tile data JSON.\n",
      "MSG: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "<tifffile.TiffFile 'Im_00001.tif'> <asarray> failed to reshape (3, 1200, 1200) to (40, 3, 1200, 1200), raised ValueError('cannot reshape array of size 4320000 into shape (40,3,1200,1200)')\n",
      "'bidirectionalCorrection' could not be loaded from tile data JSON.\n",
      "MSG: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "<tifffile.TiffFile 'Im_00002.tif'> <asarray> failed to reshape (3, 1200, 1200) to (40, 3, 1200, 1200), raised ValueError('cannot reshape array of size 4320000 into shape (40,3,1200,1200)')\n",
      "'bidirectionalCorrection' could not be loaded from tile data JSON.\n",
      "MSG: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "<tifffile.TiffFile 'Im_00003.tif'> <asarray> failed to reshape (3, 1200, 1200) to (40, 3, 1200, 1200), raised ValueError('cannot reshape array of size 4320000 into shape (40,3,1200,1200)')\n",
      "'bidirectionalCorrection' could not be loaded from tile data JSON.\n",
      "MSG: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "<tifffile.TiffFile 'Im_00004.tif'> <asarray> failed to reshape (3, 1200, 1200) to (40, 3, 1200, 1200), raised ValueError('cannot reshape array of size 4320000 into shape (40,3,1200,1200)')\n",
      "'bidirectionalCorrection' could not be loaded from tile data JSON.\n",
      "MSG: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "'bidirectionalCorrection' could not be loaded from tile data JSON.\n",
      "MSG: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "'bidirectionalCorrection' could not be loaded from tile data JSON.\n",
      "MSG: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "'bidirectionalCorrection' could not be loaded from tile data JSON.\n",
      "MSG: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "'bidirectionalCorrection' could not be loaded from tile data JSON.\n",
      "MSG: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "'bidirectionalCorrection' could not be loaded from tile data JSON.\n",
      "MSG: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "'bidirectionalCorrection' could not be loaded from tile data JSON.\n",
      "MSG: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "'bidirectionalCorrection' could not be loaded from tile data JSON.\n",
      "MSG: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "'bidirectionalCorrection' could not be loaded from tile data JSON.\n",
      "MSG: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "'bidirectionalCorrection' could not be loaded from tile data JSON.\n",
      "MSG: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "'bidirectionalCorrection' could not be loaded from tile data JSON.\n",
      "MSG: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "'bidirectionalCorrection' could not be loaded from tile data JSON.\n",
      "MSG: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "'bidirectionalCorrection' could not be loaded from tile data JSON.\n",
      "MSG: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "'bidirectionalCorrection' could not be loaded from tile data JSON.\n",
      "MSG: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "'bidirectionalCorrection' could not be loaded from tile data JSON.\n",
      "MSG: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "'bidirectionalCorrection' could not be loaded from tile data JSON.\n",
      "MSG: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "'bidirectionalCorrection' could not be loaded from tile data JSON.\n",
      "MSG: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "'bidirectionalCorrection' could not be loaded from tile data JSON.\n",
      "MSG: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "'bidirectionalCorrection' could not be loaded from tile data JSON.\n",
      "MSG: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "'bidirectionalCorrection' could not be loaded from tile data JSON.\n",
      "MSG: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "'bidirectionalCorrection' could not be loaded from tile data JSON.\n",
      "MSG: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "'bidirectionalCorrection' could not be loaded from tile data JSON.\n",
      "MSG: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "'bidirectionalCorrection' could not be loaded from tile data JSON.\n",
      "MSG: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "'bidirectionalCorrection' could not be loaded from tile data JSON.\n",
      "MSG: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "<tifffile.TiffFile 'Im_00001.tif'> <asarray> failed to reshape (31, 1200, 1200) to (40, 31, 1200, 1200), raised ValueError('cannot reshape array of size 44640000 into shape (40,31,1200,1200)')\n",
      "'bidirectionalCorrection' could not be loaded from tile data JSON.\n",
      "MSG: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "<tifffile.TiffFile 'Im_00001.tif'> <asarray> failed to reshape (31, 1200, 1200) to (40, 31, 1200, 1200), raised ValueError('cannot reshape array of size 44640000 into shape (40,31,1200,1200)')\n",
      "'bidirectionalCorrection' could not be loaded from tile data JSON.\n",
      "MSG: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "<tifffile.TiffFile 'Im_00001.tif'> <asarray> failed to reshape (31, 1200, 1200) to (40, 31, 1200, 1200), raised ValueError('cannot reshape array of size 44640000 into shape (40,31,1200,1200)')\n",
      "'bidirectionalCorrection' could not be loaded from tile data JSON.\n",
      "MSG: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'\n",
      "<tifffile.TiffFile 'Im_00001.tif'> <asarray> failed to reshape (31, 1200, 1200) to (40, 31, 1200, 1200), raised ValueError('cannot reshape array of size 44640000 into shape (40,31,1200,1200)')\n",
      "Could not find JSON associated with the image /mnt/d/data/raw/S268_250409_CAREtraining_PL/coumarin6_ExVivo_slide/Mosaic01_2x2_FOV600_z70_32Sp/Im_00002 (tileData.txt was provided as JSON extention)\n",
      "Could not initialize FLAMEImage object from /mnt/d/data/raw/S268_250409_CAREtraining_PL/coumarin6_ExVivo_slide/Mosaic01_2x2_FOV600_z70_32Sp/Im_00002.tif\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "for root, dirs, files in os.walk(INFERENCE_DATA_DIR):\n",
    "    for f in files:\n",
    "        if \"tif\" in f or \"tiff\" in f:\n",
    "            try:\n",
    "                this_image = FLAMEImage(\n",
    "                    impath = os.path.join(root, f),\n",
    "                    jsonext = \"tileData.txt\",\n",
    "                    # overrideNFrames = 1,\n",
    "                    checkFrames = True,\n",
    "                    checkZs = False,\n",
    "                    requireBidirectionalCorrection=False\n",
    "                )\n",
    "            except FLAMEImageError as e:\n",
    "                continue\n",
    "            images.append(this_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "220c61b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a7e453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = []\n",
    "axes = []\n",
    "for im in images:\n",
    "    if im.imShape not in shapes:\n",
    "        shapes.append(im.imShape)\n",
    "    if im.axes_shape not in axes:\n",
    "        axes.append(im.axes_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9ca1c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 1200, 1200), (3, 7700, 1100), (40, 3, 1200, 1200), (31, 1200, 1200)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90c4c0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CYX', 'FCYX']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a55d7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CARE(config=None, name=MODEL_DIRECTORY)\n",
    "engine = CAREInferenceSession(\n",
    "    model_path=ONNX_PATH,\n",
    "    model_config_path=JSON_PATH,\n",
    "    dataset_config_path=JSON_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de260e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CYX (3, 7700, 1100)\n",
      "FCYX (40, 3, 1200, 1200)\n",
      "CYX (31, 1200, 1200)\n"
     ]
    }
   ],
   "source": [
    "print(images[10].axes_shape, images[10].imShape)\n",
    "print(images[20].axes_shape, images[20].imShape)\n",
    "print(images[30].axes_shape, images[30].imShape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab39aa30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 3, 1]\n",
      "(np.int64(1), 7700, 1100, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not infer on array of shape (549, 128, 128, 1) and dtype float32.\n",
      "ERROR: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: patch for the following indices\n",
      " index: 3 Got: 1 Expected: 3\n",
      " Please fix either the inputs/outputs or the model.\n"
     ]
    },
    {
     "ename": "CAREInferenceError",
     "evalue": "Could not infer on array of shape (549, 128, 128, 1) and dtype float32.\nERROR: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: patch for the following indices\n index: 3 Got: 1 Expected: 3\n Please fix either the inputs/outputs or the model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/mnt/d/code/Balu_CARE/flame/engine.py:106\u001b[0m, in \u001b[0;36mCAREInferenceSession.predict\u001b[0;34m(self, arr)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minferenceSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43marr\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/care/lib/python3.12/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:273\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m C\u001b[38;5;241m.\u001b[39mEPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: patch for the following indices\n index: 3 Got: 1 Expected: 3\n Please fix either the inputs/outputs or the model.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCAREInferenceError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pred_CYX \u001b[38;5;241m=\u001b[39m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_FLAME\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m pred_FCYX \u001b[38;5;241m=\u001b[39m engine\u001b[38;5;241m.\u001b[39mpredict_FLAME(images[\u001b[38;5;241m20\u001b[39m])\n\u001b[1;32m      3\u001b[0m pred_CYX2 \u001b[38;5;241m=\u001b[39m engine\u001b[38;5;241m.\u001b[39mpredict_FLAME(images[\u001b[38;5;241m30\u001b[39m])\n",
      "File \u001b[0;32m/mnt/d/code/Balu_CARE/flame/engine.py:194\u001b[0m, in \u001b[0;36mCAREInferenceSession.predict_FLAME\u001b[0;34m(self, image, input_frames)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cdx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(frames\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]): \u001b[38;5;66;03m# now loopoing through the channel dimensions to predict 1 by 1.\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     patches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_patches(n[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,[cdx]]) \u001b[38;5;66;03m# keep dimension while indexing\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m     channel_output\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stitch_patches(output, image\u001b[38;5;241m.\u001b[39m_get_dims(axes\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYXC\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[1;32m    196\u001b[0m full_output\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mstack(channel_output, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/mnt/d/code/Balu_CARE/flame/engine.py:109\u001b[0m, in \u001b[0;36mCAREInferenceSession.predict\u001b[0;34m(self, arr)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not infer on array of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marr\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mERROR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CAREInferenceError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not infer on array of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marr\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mERROR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mCAREInferenceError\u001b[0m: Could not infer on array of shape (549, 128, 128, 1) and dtype float32.\nERROR: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: patch for the following indices\n index: 3 Got: 1 Expected: 3\n Please fix either the inputs/outputs or the model."
     ]
    }
   ],
   "source": [
    "pred_CYX = engine.predict_FLAME(images[10])\n",
    "pred_FCYX = engine.predict_FLAME(images[20])\n",
    "pred_CYX2 = engine.predict_FLAME(images[30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8e4604",
   "metadata": {},
   "outputs": [],
   "source": [
    "images[20].get_frames((0,5)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7232c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_im = images[0].raw()[0,...].transpose(1,2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97219cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60dae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = engine._get_patches(test_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6358d516",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0a7330",
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0].imShape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5cbe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure = plt.figure(figsize=(10, 10))\n",
    "# for pdx, patch in enumerate(patches):\n",
    "#     ax = figure.add_subplot(10, 10, pdx+1)\n",
    "#     ax.imshow(patch)\n",
    "#     ax.set_axis_off()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba434db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "restitched = engine._stitch_patches(patches, (1200,1200,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e331d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all((test_im == restitched))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a1b30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flame.utils import min_max_norm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf8517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load(open(JSON_PATH, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28484f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1pct = config[\"FLAME_Dataset\"][\"input\"][\"pixel_1pct\"]\n",
    "input_99pct = config[\"FLAME_Dataset\"][\"input\"][\"pixel_99pct\"]\n",
    "output_1pct = np.array(config[\"FLAME_Dataset\"][\"output\"][\"pixel_1pct\"])\n",
    "output_99pct = np.array(config[\"FLAME_Dataset\"][\"output\"][\"pixel_99pct\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b273a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = np.clip(patches, a_min=input_1pct, a_max=output_99pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d40534",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = min_max_norm(patches, mini=input_1pct, maxi=input_99pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7fd78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = engine.predict(patches.astype(np.float32))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c170b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee128844",
   "metadata": {},
   "outputs": [],
   "source": [
    "denoised = engine._stitch_patches(pred, (1200,1200,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ccd108",
   "metadata": {},
   "outputs": [],
   "source": [
    "denoised = denoised * (output_99pct - output_1pct) + output_1pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d66b3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "denoised = (denoised - denoised.min()) / (denoised.max() - denoised.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093dfa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "denoised.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3779fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(denoised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e87f7f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53123b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = json.load(open(DATASET_JSON, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77e6ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_min = np.array(dataset_dict['FLAME_Dataset']['input']['pixel_1pct'])\n",
    "input_max = np.array(dataset_dict['FLAME_Dataset']['input']['pixel_99pct'])\n",
    "output_min = np.array(dataset_dict['FLAME_Dataset']['output']['pixel_1pct'])\n",
    "output_max = np.array(dataset_dict['FLAME_Dataset']['output']['pixel_99pct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794c8f09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318eaac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in images:\n",
    "    image.openImage()\n",
    "    model_input = image.imageData.copy()\n",
    "    image.closeImage()\n",
    "    model_input = model_input.transpose(0, 2, 3, 1)\n",
    "    model_input = np.clip(model_input, input_min, input_max)\n",
    "    model_input = min_max_norm(model_input, input_min, input_max, dtype=np.float32)\n",
    "    outputs = []\n",
    "    for zdx in range(model_input.shape[0]):\n",
    "        outputs.append(model.predict(model_input[zdx,...], axes='YXC'))\n",
    "    outputs = np.stack(outputs, axis = -1).transpose(3, 0, 1, 2)\n",
    "    outputs = (outputs * (output_max - output_min + 1e-20)) + output_min\n",
    "    outputs[...,0] *= 4/3\n",
    "    outputs[...,2] *= 2/3\n",
    "    imname = \"_\".join(image.impath.split(os.path.sep)[4:])\n",
    "    os.makedirs(os.path.join(INFERENCE_OUTPUT_DIRECTORY, os.path.splitext(imname)[0]))\n",
    "    tiff.imwrite(os.path.join(INFERENCE_OUTPUT_DIRECTORY, imname), outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d649a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = \"/mnt/c/Users/BaluLab/Desktop/raw_vis\"\n",
    "os.makedirs(DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ef5459",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, image in enumerate(images):\n",
    "    image.openImage()\n",
    "    raw = image.imageData.copy()\n",
    "    image.closeImage()\n",
    "    raw = raw.transpose(0, 2, 3, 1).astype(np.float32)\n",
    "    raw[...,0] *= 4/3\n",
    "    raw[...,2] *= 2/3\n",
    "    raw = min_max_norm(raw, np.min(raw), np.max(raw), dtype=np.float32)\n",
    "    raw = (raw * 255).astype(np.uint8)\n",
    "    for zdx in [0, 4, 9]:\n",
    "        impath = os.path.join(DIR, f\"im_i{idx}_z{zdx}.tif\")\n",
    "        tiff.imwrite(os.path.join(impath), raw[zdx,...])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff470d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "care",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
